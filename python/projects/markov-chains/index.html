<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Markov Chains</title><meta name=viewport content="width=device-width"><link rel=stylesheet href=https://DrCS.codes/css/syntax.css><link rel=stylesheet href=https://DrCS.codes/css/main.css><link rel=stylesheet href=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css integrity=sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO crossorigin=anonymous><link href=/css/navbar-fixed-top.css rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script><script type=text/x-mathjax-config>
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i &lt; all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script></head><body><nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"><a class=navbar-brand href=/python></a>
<span class=navbar-toggler-icon></span></a><div class="navbar-collapse collapse" id=navbarCollapse><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link onclick=history.back()>Back to course</a></li></ul><ul class="navbar-nav pull-right"><li class="nav-item pull-right"><a class=nav-link href=https://DrCS.codes>Dr. CS codes</a></li></ul></div></nav><div class=container><h1 id=markov-chains>Markov Chains</h1><h2 id=introduction>Introduction</h2><p>In this assignment you&rsquo;ll practice</p><ul><li>writing classes and modules,</li><li>simple text processing, and</li><li>basic numerical computing issues in Python.</li></ul><h2 id=problem-description>Problem Description</h2><p>You&rsquo;re interested in natural language processing, and in the problem of identifying the source of a text.</p><h2 id=solution-description>Solution Description</h2><p>Write a module named <code>source.py</code> with a class named <code>SourceModel</code> whose constructor takes a name for a source, and a courpus object of type <code>TextIOWrapper</code> (such as a file object &ndash; see <a href=https://docs.python.org/3/library/io.html>io module</a>) and builds a first-order Markov model of the transitions between letters in the source. Only alphabetic characters in the source corpus should be considered and they should be normalized to upper or lower case. For simplicity (see background) only consider the 26 letters of the English alphabet.</p><p>Here are some example corpus files and test files:</p><ul><li>English: <a href=english.corpus>english.corpus</a>, <a href=english.test>english.test</a></li><li>French: <a href=french.corpus>french.corpus</a>, <a href=french.test>french.test</a></li><li>Spanish: <a href=spanish.corpus>spanish.corpus</a>, <a href=spanish.test>spanish.test</a></li><li>HipHop: <a href=hiphop.corpus>hiphop.corpus</a>, <a href=hiphop.test>hiphop.test</a></li><li>Lisp: <a href=lisp.corpus>lisp.corpus</a>, <a href=lisp.test>lisp.test</a></li></ul><p>You can assume corpus files are of the form <code>&lt;source-name>.corpus</code>.</p><h3 id=background>Background</h3><p>In machine learning we train a model on some data and then use that model to make predictions about unseen instance of the same kind of data. For example, we can train a machine learning model on a data set consisting of several labeled images, some of which depict a dog and some of which don&rsquo;t. We can then use the trained model to predict whether some unseen image (an image not in the training set) has a dog. The better the model, the better the accuracy (percentage of correct predictions) on unseen data.</p><p>We can create a model of language and use that model to predict the likelihood that some unseen text was generated by that model, in other words, how likely the unseen text is an example of the language modeled by the model. The model could be of a particular author, or a language such as French or English. One simple kind of model is well-suited to this problem: Markov models.</p><p>Markov models are usefule for time-series or other sequential data. A Markov model is a finite-state model in which the current state is dependent only on a bounded history of previous states. In a first-order Markov model the current state is dependent on only one previous state.</p><p>One can construct a simple first-order Markov model of a language as the transition probablilities between letters in the language&rsquo;s alphabet. For example, given this training corpus of a language:</p><pre tabindex=0><code>BIG A, little a, what begins with A?
Aunt Annieâ€™s alligator.
A...a...A

BIG B, little b, what begins with B?
Barber, baby, bubbles and a bumblebee.
</code></pre><p>We would have the model:</p><p>TODO: resize image.</p><p><img src=drseuss-model.png alt="Dr Seuss Language Model"></p><p>We&rsquo;ve only shown the letter-to-letter transitions that occur in the training corpus. Notice:</p><ul><li>we&rsquo;ve normalized to lowercase,</li><li>we only consider letters &ndash; whitespace and punctuation are ignored</li><li>the transition probabilities from one letter to all other letters sum to 1.0 (approximately, due to rounding),</li><li>in a complete model, there would be arrows leading from all letters to all other letters, for $26^26$ edges in a graph of 26 nodes, one for each letter, with unseen transitions labeled with 0.</li></ul><p>This model indicates that whenever the letter a occurs in our training corpus, the next letter is a, b, e, g, h, i, l, n, r, s, t, u or w. The arrow from a to b is labeled .19 because b appears after a 3 out of 16 times, or approximately 19 percent of the time. A first-order Markov chain is a kind of bigram model. Here are all the bigrams in the training text that begin with a, that is, all the state transitions from a:</p><pre tabindex=0><code>(a, l), (a, w), (a, t), (a, a), (a, u), (a, n), (a, l), (a, t),
(a, a), (a, a), (a, b), (a, t), (a, r), (a, b), (a, n), (a, b)
</code></pre><p>A Markov chain represents all the bigrams and their probabilities of occurence in the training corpus.</p><h4 id=representation-as-a-matrix>Representation as a matrix.</h4><p>A Markov chain can be represented as a transition matrix in which the probability of state j after state i is found in element (i, j) of the matrix. In the example below we have labeled the rows and columns with letters for readability. The probability of seeing the letter n after the letter a in the training corpus is found by entering row a and scanning accross to column n, where we find the probability .12.</p><table><thead><tr><th></th><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th><th>f</th><th>g</th><th>h</th><th>i</th><th>j</th><th>k</th><th>l</th><th>m</th><th>n</th><th>o</th><th>p</th><th>q</th><th>r</th><th>s</th><th>t</th><th>u</th><th>v</th><th>w</th><th>x</th><th>y</th><th>z</th></tr></thead><tbody><tr><td><strong>a</strong></td><td>0.19</td><td>0.19</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.12</td><td>0.01</td><td>0.12</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.06</td><td>0.01</td><td>0.19</td><td>0.06</td><td>0.01</td><td>0.06</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>b</strong></td><td>0.12</td><td>0.12</td><td>0.01</td><td>0.01</td><td>0.24</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.12</td><td>0.01</td><td>0.01</td><td>0.18</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.12</td><td>0.01</td><td>0.06</td><td>0.01</td><td>0.06</td><td>0.01</td></tr><tr><td><strong>c</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>d</strong></td><td>1.00</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>e</strong></td><td>0.11</td><td>0.22</td><td>0.01</td><td>0.01</td><td>0.11</td><td>0.01</td><td>0.22</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.11</td><td>0.22</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>f</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>g</strong></td><td>0.40</td><td>0.20</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.40</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>h</strong></td><td>0.75</td><td>0.25</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>i</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.10</td><td>0.01</td><td>0.30</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.20</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.40</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>j</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>k</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>l</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.50</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.38</td><td>0.01</td><td>0.01</td><td>0.12</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>m</strong></td><td>0.01</td><td>1.00</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>n</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.17</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.17</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.17</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.33</td><td>0.17</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>o</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>1.00</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>p</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>q</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>r</strong></td><td>0.33</td><td>0.67</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>s</strong></td><td>0.50</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.50</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>t</strong></td><td>0.10</td><td>0.20</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.20</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.20</td><td>0.01</td><td>0.01</td><td>0.10</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.20</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>u</strong></td><td>0.01</td><td>0.33</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.33</td><td>0.33</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>v</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>w</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.50</td><td>0.50</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>x</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>y</strong></td><td>0.01</td><td>1.00</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><strong>z</strong></td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr></tbody></table><h4 id=prediction-using-a-markov-model>Prediction using a Markov model.</h4><p>Given a Markov chain model of a source, we can compute the probability that the model would produce a given string of letters by applying the chain rule. Simply stated, we walk the transitions in the Markov chain and multiply the trasition probabilities. For example, the rpobability that &ldquo;Big C, Little C&rdquo; would be produced by our model, we would get the following probabilities from the transition matrix:</p><pre tabindex=0><code>p(b, i) = .12
p(i, g) = .30
p(g, c) = .01
p(c, l) = .01
p(l, i) = .38
p(i, t) = .40
p(t, t) = .20
p(t, l) = .20
p(l, e) = .50
p(e, c) = .01
</code></pre><p>Multiplying them gives us 1.0588235294117648e-10. Notice that, in order to avoid getting zero-probability predictions using our simplified technique, we store .01 in our transition matrix for any bigram we don&rsquo;t see in the training corpus.</p><h4 id=additional-information>Additional Information</h4><p>We&rsquo;ve greatly simplified the presentation here to focus on the computer programming. For more information consult the following references.</p><ul><li>Natural language processing: <a href=https://web.stanford.edu/~jurafsky/slp3/>https://web.stanford.edu/~jurafsky/slp3/</a></li><li>Markov chains: Chapter 11 of <a href=http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book.html>http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book.html</a>, direct link: <a href=https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf>https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf</a></li></ul><h3 id=requirements>Requirements</h3><p>Here&rsquo;s a skeleton <code>source.py</code> to get you started:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SourceModel</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, name, text_stream):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Recommended algorithm for building your transition matrix:</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Initialize a 26x26 matrix (e.g., 26-element list of 26-element lists)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Print &#34;Training {lang} model ... &#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Read the text_stream one character at a time.</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># For each character, increment the correscponding (row, col) in your</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># matrix. The row is the for the previous character, the col is for</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># the current character. (You could also think of this in terms of bigrams.)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># After you read the entire text_stream, you&#39;ll have a matrix of counts.</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># From the matrix of counts, create a matrix of probabilities --</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># each row of the transition matrix is a probability distribution.</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Print &#34;done.&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __repr__(self):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>probability</span>(self, text_stream):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__<span style=color:#f92672>==</span><span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>   <span style=color:#75715e># The first n-1 arguments to the script are corpus files to train models.</span>
</span></span><span style=display:flex><span>   <span style=color:#75715e># Corupus files are of the form &lt;source-name&gt;.corpus</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>   <span style=color:#75715e># The last argument to the script is either a file to analyze or a sentence.</span>
</span></span><span style=display:flex><span>   <span style=color:#75715e># Sentences should be in quotes. (Same with file names that have special characters</span>
</span></span><span style=display:flex><span>   <span style=color:#75715e># or spaces, but decent people don&#39;t put special characters or spaces in file names.)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>   <span style=color:#75715e># Create a SourceModel object for each corpus</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>   <span style=color:#75715e># Use the models to compute the probability that the test text was produced by the model</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>   <span style=color:#75715e># Probabilities will be very small. Normalize the probablilities of all the model</span>
</span></span><span style=display:flex><span>   <span style=color:#75715e># predictions to a probability distribution (so they sum to 1)</span>
</span></span><span style=display:flex><span>   <span style=color:#75715e># (closed-world assumption -- we only state probabilities relative to models we have).</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>   <span style=color:#75715e># Print results of analysis, sorted in order from most likely to least likely</span>
</span></span></code></pre></div><h2 id=running-your-script>Running Your Script</h2><p>Here&rsquo;s an analysis of a line from a famous rap song:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ python source.py *.corpus <span style=color:#e6db74>&#34;If you got a gun up in your waist please don&#39;t shoot up the place (why?)&#34;</span>
</span></span><span style=display:flex><span>Training english model ... <span style=color:#66d9ef>done</span>.
</span></span><span style=display:flex><span>Training french model ... <span style=color:#66d9ef>done</span>.
</span></span><span style=display:flex><span>Training hiphop model ... <span style=color:#66d9ef>done</span>.
</span></span><span style=display:flex><span>Training lisp model ... <span style=color:#66d9ef>done</span>.
</span></span><span style=display:flex><span>Training spanish model ... <span style=color:#66d9ef>done</span>.
</span></span><span style=display:flex><span>Analyzing string If you got a gun up in your waist please don<span style=color:#960050;background-color:#1e0010>&#39;</span>t shoot up the place <span style=color:#f92672>(</span>why?<span style=color:#f92672>)</span>.
</span></span><span style=display:flex><span>Relative probability that test string is hiphop  : 0.998103661.
</span></span><span style=display:flex><span>Relative probability that test string is english : 0.001896339.
</span></span><span style=display:flex><span>Relative probability that test string is french  : 0.000000000.
</span></span><span style=display:flex><span>Relative probability that test string is spanish : 0.000000000.
</span></span><span style=display:flex><span>Relative probability that test string is lisp    : 0.000000000.
</span></span></code></pre></div><pre tabindex=0><code>$ python source.py *.corpus &#34;Ou va le monde&#34;
Training english model ... done.
Training french model ... done.
Training hiphop model ... done.
Training lisp model ... done.
Training spanish model ... done.
Analyzing string Ou va le monde.
Relative probability that test string is french  : 0.904369735.
Relative probability that test string is lisp    : 0.064067191.
Relative probability that test string is english : 0.014523700.
Relative probability that test string is hiphop  : 0.009526881.
Relative probability that test string is spanish : 0.007512493.
</code></pre></div><script src=js/bootstrap.min.js></script></body></html>