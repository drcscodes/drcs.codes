<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Dr. CS codes</title><meta name=viewport content="width=device-width"><link rel=stylesheet href=https://DrCS.codes/css/syntax.css><link rel=stylesheet href=https://DrCS.codes/css/main.css><link rel=stylesheet href=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css integrity=sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO crossorigin=anonymous><link href=https://DrCS.codes/css/navbar-top-fixed.css rel=stylesheet></head><body><nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"><a class=navbar-brand href=/machine-learning></a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbarCollapse aria-controls=navbarCollapse aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="navbar-collapse collapse" id=navbarCollapse><ul class="navbar-nav mr-auto"></ul><ul class="navbar-nav pull-right"><li class="nav-item pull-right"><a class=nav-link href=https://DrCS.codes>Dr. CS codes</a></li></ul></div></nav><main role=main class=container><p>% Linear Regression</p><h2 id=the-linear-model>The Linear Model</h2><p>So far we&rsquo;ve dealt with classification, where the target function maps feature vectors to discrete classes, but the linear model is more versatile. Consider the credit analysis problem:</p><p><img src=lfd-ex3_4-linear-tasks.png alt>{height=30%}</p><p>We can use the linear model to learn</p><ul><li>a yes/no (perceptron)</li><li>an arbitrary real number (linear regression)</li><li>a probability (logistic regression)</li></ul><p>As we&rsquo;ll see later, we can even learn to separate classes that are not linearly separable due to their nature, not noise in the data set.</p><h2 id=the-linear-signal>The Linear Signal</h2><p><img src=lfd-magdon-ismail-linear-signal.png alt>{height=40%}<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>$$
y = \theta(s)
$$</p><p>Three different error measures/loss functions:</p><ul><li>Perceptron: Classification error (0-1 loss)</li><li>Linear Regression: Mean square error</li><li>Logistic Regression: Cross-entropy error</li></ul><p>The different error measures lead to different algorithms for minimizing the error.</p><h2 id=linear-regression>Linear Regression</h2><p>In linear regression the target function maps feature vectors in $\mathbb{R}^{d+1}$ to arbitrary real values.</p><p><img src=september-sea-ice.png alt>{height=40%}<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><ul><li>In linear binary classification we assume that there is a line that separates classes acceptably well.</li><li>In linear regression we assume that there is a line that fits the data acceptably well.</li></ul><h2 id=error-for-linear-regression>Error for Linear Regression</h2><p>In linear regression we minimize the mean square error (MSE) between $h(\vec{x})$ and $y$.</p><p>$$
E_{in} = \frac{1}{N} \sum_{n=1}^{N} ( h(\vec{x_n} - y_n )^2
$$</p><p><img src=lfd-fig3_3-linear-regression-error.png alt>{height=50%}</p><h2 id=matrix-representation-of-e_invecw>Matrix Representation of $E_{in}(\vec{w})$</h2><p>We can represent the problem in matrix form:</p><p><img src=lfd-magdon-matrix-linear-regression.png alt>{height=40%}</p><p>Then:</p><p><img src=lfd-magdon-regression-error-derivation.png alt>{height=40%}</p><h2 id=minimizing-e_invecw>Minimizing $E_{in}(\vec{w})$</h2><p>For linear regression, $h$ is a linear combination of the components of $\vec{x}$:</p><p>$$
h(x) = \vec{w}^T \vec{x}
$$</p><p>And minimizing the error is expressed as the optimization problem:</p><p>$$
w_{lin} = \argmin_{\vec{w} \in \mathbb{R}^{d+1}} E_{in}(\vec{w})
$$</p><h2 id=minimizing-matrix-represeantation-of-e_invecw>Minimizing Matrix Represeantation of $E_{in}(\vec{w})$</h2><p>Since $E_{in}(\vec{w})$ is differentiable we can set the gradient to 0:</p><p>$$
\nabla E_{in}(\vec{w}) = \vec{0}
$$</p><p>The gradient of our matrix representation of $E_{in}(\vec{w})$ is</p><p>$$
\nabla E_{in}(\vec{w}) = \frac{2}{N} ( X^T X \vec{w} - X^T \vec{y})
$$</p><p>which is $\vec{0}$ when</p><p>$$
X^T X \vec{w} = X^T \vec{y}
$$</p><p>If we assume $X^T X$ is invertible, then $\vec{w} = X^{\dagger} \vec{y}$, leading to the one-step algorithm for linear regression &mldr;</p><h2 id=linear-regression-algorithm>Linear Regression Algorithm</h2><p><img src=lfd-linear-regression-algorithm.png alt>{height=80%}</p><h2 id=closing-thoughts>Closing Thoughts</h2><ul><li><p>The linear regression algorithm demonstrates some common themes in machine learning:</p><ul><li>manipulate the error function into a form that allows us to use to mathematical tricks to simplify the problem</li><li>make simplifying assumptions that make the theory clean but typically work well in practice</li></ul></li><li><p>Linear regression is well-studied in statistics</p></li><li><p>Some wouldn&rsquo;t consider linear regression to be machine learning becuase it has an analytic rather than algorithmic solution</p><ul><li>important ideas in linear regression appear in other algorithms (e.g., minimizing gradient)</li><li>linear regression is an important tool in a data scientist&rsquo;s tool box</li></ul></li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=http://www.cs.rpi.edu/~magdon/courses/learn/slides.html>http://www.cs.rpi.edu/~magdon/courses/learn/slides.html</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://www.cc.gatech.edu/~bboots3/CS4641-Fall2018/Lecture3/03_LinearRegression.pdf>https://www.cc.gatech.edu/~bboots3/CS4641-Fall2018/Lecture3/03_LinearRegression.pdf</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></main><script src=https://code.jquery.com/jquery-3.2.1.slim.min.js integrity=sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js integrity=sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q crossorigin=anonymous></script>
<script src=../js/bootstrap.min.js></script></body></html>