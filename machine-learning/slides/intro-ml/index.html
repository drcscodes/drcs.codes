<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Dr. CS codes</title><meta name=viewport content="width=device-width"><link rel=stylesheet href=https://DrCS.codes/css/syntax.css><link rel=stylesheet href=https://DrCS.codes/css/main.css><link rel=stylesheet href=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css integrity=sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO crossorigin=anonymous><link href=https://DrCS.codes/css/navbar-top-fixed.css rel=stylesheet></head><body><nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"><a class=navbar-brand href=/machine-learning></a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbarCollapse aria-controls=navbarCollapse aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="navbar-collapse collapse" id=navbarCollapse><ul class="navbar-nav mr-auto"></ul><ul class="navbar-nav pull-right"><li class="nav-item pull-right"><a class=nav-link href=https://DrCS.codes>Dr. CS codes</a></li></ul></div></nav><main role=main class=container><p>% Introduction to Machine Learning</p><h2 id=what-is-learning>What is learning?</h2><blockquote><p>&ldquo;Learning is any process by which a system improves performance from experience.&rdquo;</p><ul><li>Herbert Simon</li></ul></blockquote><h2 id=what-is-machine-learning>What is machine learning?</h2><p>The hype answer:</p><blockquote><p>&ldquo;Machine learning is the next Internet&rdquo;</p><ul><li>Tony Tether, Director, DARPA</li></ul></blockquote><blockquote><p>&ldquo;Machine learning is the hot new thing&rdquo;</p><ul><li>John Hennessy, President, Stanford</li></ul></blockquote><blockquote><p>&ldquo;Machine learning is today’s discontinuity&rdquo;</p><ul><li>Jerry Yang, CEO, Yahoo</li></ul></blockquote><blockquote><p>&ldquo;Machine learning is the new electricity&rdquo;
-Andrew Ng, Chief Scientist Baidu</p></blockquote><blockquote><p>&ldquo;Software is eating the world.&rdquo;</p><ul><li>Marc Andreessen, co-Author of Mosaic, founder of Andreessen Horowitz (VC firm).</li></ul></blockquote><blockquote><p>&ldquo;AI is eating software.&rdquo;</p><ul><li>Jensen Huang, CEO Nvidia</li></ul></blockquote><p><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><h2 id=what-is-machine-learning-1>What is machine learning?</h2><p>The hype is real.</p><p><img src=target-pregnant.jpg alt>{height=70%}</p><p><a href=https://slate.com/human-interest/2014/06/big-data-whats-even-creepier-than-target-guessing-that-youre-pregnant.html>From slate.com</a></p><h2 id=what-is-machine-learning-2>What is machine learning?</h2><p>Definition by Tom Mitchell (1998):</p><p>Machine Learning is the study of algorithms that</p><ul><li>improve their performance <code>P</code></li><li>at some task <code>T</code></li><li>with experience <code>E</code>.</li></ul><p>A well-defined learning task is given by <code>&lt;P, T, E></code>.</p><h2 id=examples-of-machine-learning-tasks-mooney>Examples of Machine Learning Tasks <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></h2><p>Improve on task <code>T</code>, with performance <code>P</code>, given experience <code>E</code></p><p><code>T</code>: Playing checkers</p><ul><li><code>P</code>: Percentage of games won against an arbitrary opponent</li><li><code>E</code>: Playing practice games against itself</li></ul><p><code>T</code>: Recognizing hand-written words</p><ul><li><code>P</code>: Percentage of words correctly classified</li><li><code>E</code>: Database of human-labeled images of handwritten words</li></ul><p><code>T</code>: Driving on four-lane highways using vision sensors</p><ul><li><code>P</code>: Average distance traveled before a human-judged error</li><li><code>E</code>: A sequence of images and steering commands recorded while observing a human driver.</li></ul><p><code>T</code>: Categorize email messages as spam or legitimate.</p><ul><li><code>P</code>: Percentage of email messages correctly classified.</li><li><code>E</code>: Database of emails, some with human-given labels</li></ul><h2 id=elements-of-machine-learning-tasks>Elements of Machine Learning Tasks</h2><p>Every machine learning task includes</p><ul><li>data from which to learn,</li><li>a model that takes input data and produces an &ldquo;answer&rdquo;,</li><li>a loss function that quantifies the badness of our model, and</li><li>an algorithm that adjiusts the model’s parameters to minimize the loss</li></ul><p>Our model, or hypothesis, comes from a model/hypothesis class. Once the parameters are learned, we have an instance of the hypothesis class tuned to our particular machine learning problem.</p><h2 id=machine-learning-archtecture>Machine Learning Archtecture</h2><p><img src=ml-architecture.png alt>{height=80%}</p><h2 id=traditional-programming-vs-machine-learning>Traditional Programming vs Machine Learning</h2><p><img src=program-box-diagram.png alt>{height=80%}</p><h2 id=when-to-use-machine-learning>When to use Machine Learning</h2><p>ML is used when <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>:</p><ul><li>Human expertise does not exist (navigating on Mars)</li><li>Humans can’t explain their expertise (speech recognition, vision)</li><li>Models must be customized (personalized medicine)</li><li>Models are based on huge amounts of data (genomics)</li></ul><h2 id=kinds-of-machine-learning-tasks>Kinds of Machine Learning Tasks</h2><ul><li><p>Classification: identify the correct label for an instance</p><ul><li>Is this a picture of a dog?</li><li>Which radio emitted the signal we received?</li><li>Will this customer respond to this advertisement?</li></ul></li><li><p>Clustering: identify the groups into which instances fall</p><ul><li>What are the discernible groups of &mldr; customers, cars, colors in an images</li></ul></li><li><p>Agent behavior</p><ul><li>Given the state, which action should the agent take to maximize its goal attainment?</li></ul></li></ul><h2 id=categories-of-machine-learning-algorithms>Categories of Machine Learning Algorithms</h2><ul><li><p>Supervised</p><ul><li>Learn from a training set of labeled data &ndash; the supervisor</li><li>Generalize to unseen instances</li></ul></li><li><p>Unsupervised</p><ul><li>Learn from a set of unlabeled data</li><li>Place an unseen instance into appropriate group</li><li>Infer rules describing the groups</li></ul></li><li><p>Reinforcement learning</p><ul><li>Learn from a history of trial-and-error exploration</li><li>Output is a /policy/ &ndash; a mapping from states to actions (or probabolity distributions over actions)</li></ul></li></ul><p>Classification using supervised learning methods makes up the lion&rsquo;s share of machine learning.</p><h2 id=supervised-learning-example-breast-cancer-identification>Supervised Learning Example: Breast Cancer Identification</h2><ul><li>Given $(\vec{x_1}, y_1), (\vec{x_2}, y_2), &mldr;, (\vec{x_n}, y_n)$</li><li>Learn a function $h(\vec{x})$ to approximate $f(\vec{x})$ &ndash; the <em>target</em> function &ndash; to predict $y$ given $\vec{x}$
– if $y$ is categorical == classification
– if $y$ is real-valued == regression</li></ul><p><img src=breast-cancer-univariate.png alt>{height=40%}<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></p><h2 id=supervised-learning-example-breast-cancer-identification-1>Supervised Learning Example: Breast Cancer Identification</h2><p>We can often obtain a better model with more features.</p><p><img src=breast-cancer-multivariate.png alt>{height=60%}</p><p>But as we&rsquo;ll learn, we don&rsquo;t get this extra power for free.</p><h2 id=state-of-the-art-in-machine-learning----autonomous-cars>State of the Art in Machine Learning &ndash; Autonomous Cars</h2><p><img src=thrun-autonomous-cars.png alt>{height=80%}</p><h2 id=state-of-the-art-in-machine-learning----scene-labeling>State of the Art in Machine Learning &ndash; Scene Labeling</h2><p><img src=farabet-scene-labeling.png alt>{width=95%}<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup></p><h2 id=state-of-the-art-in-machine-learning----speech-recognition>State of the Art in Machine Learning &ndash; Speech Recognition</h2><p><img src=zeiler-speech-recognition.png alt>{height=80%}</p><h2 id=a-practical-example-iris-classification>A Practical Example: Iris Classification</h2><p>It&rsquo;s a rite of passage to apply supervised learning to the Iris data set. The canonical source for the Iris data set is the <a href=https://archive.ics.uci.edu/ml/>UCI Machine Learning Repository</a>. Download <a href=https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data>iris.data</a>.</p><p><img src=iris.jpg alt>{height=6-%}</p><h2 id=iris-data>Iris Data</h2><p>The data set contains 150 instance of Iris flowers with</p><ul><li>4 features:<ul><li>sepal_length</li><li>sepal_width</li><li>petal_length</li><li>petal_width</li></ul></li></ul><p>and</p><ul><li>3 classes:<ul><li>Iris-setosa</li><li>Iris-versicolour</li><li>Iris-virginica</li></ul></li></ul><h2 id=scikit-learn-recipe>Scikit-learn Recipe</h2><ol><li>Set up feature matrix and target array</li><li>Choose (import) model class</li><li>Set model parameters via arguments to model constructor</li><li>Fit model to data</li><li>Apply model to new data</li></ol><p>Let&rsquo;s apply this recipe to a data set.</p><h2 id=scikit-learn-data-representation>Scikit-learn Data Representation</h2><p>The basic supervised learning setup in Scikit-learn is:</p><ul><li><p>Feature Matrix</p><ul><li>Rows are instances</li><li>Columns are features</li></ul></li><li><p>Target array</p><ul><li>An array of len(rows) containing the training labels for each instance</li></ul></li></ul><p>We can easily obtain these with a Pandas DataFrame.</p><h2 id=step-1-iris-feature-matix-and-target-array>Step 1: Iris feature matix and target array</h2><p>From the description on the <a href=https://archive.ics.uci.edu/ml/datasets/Iris>Iris Data Set page</a> we know that the Iris instances have four features &ndash; (sepal_length, sepal_width, petal_length, petal_width) &ndash; and three classes &ndash; (Iris-setosa, Iris-versicolour, Iris-virginica). We can read these into a DataFrame with</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sklearn
</span></span><span style=display:flex><span>iris <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;iris.data&#34;</span>, names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;sepal_length&#34;</span>,
</span></span><span style=display:flex><span>                                       <span style=color:#e6db74>&#34;sepal_width&#34;</span>,
</span></span><span style=display:flex><span>                                       <span style=color:#e6db74>&#34;petal_length&#34;</span>,
</span></span><span style=display:flex><span>                                       <span style=color:#e6db74>&#34;petal_width&#34;</span>,
</span></span><span style=display:flex><span>                                       <span style=color:#e6db74>&#34;species&#34;</span>])
</span></span></code></pre></div><p>Note: this DataFrame is in <a href=http://vita.had.co.nz/papers/tidy-data.pdf>tidy format</a>.</p><h2 id=step-11-scikit-learn-input-data>Step 1.1: Scikit-learn Input Data</h2><p>For Scikit-learn we need a feature matrix <code>X</code> and target array <code>y</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>X_iris <span style=color:#f92672>=</span> iris<span style=color:#f92672>.</span>drop(<span style=color:#e6db74>&#34;species&#34;</span>, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>y_iris <span style=color:#f92672>=</span> iris[<span style=color:#e6db74>&#34;species&#34;</span>]
</span></span></code></pre></div><p>We can check that the number of samples in the feature matrix equals the number of labels in the target array with</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>X_iris<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>==</span> y_iris<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#75715e># True</span>
</span></span></code></pre></div><p>There are 150 samples and 150 target labels.</p><h2 id=step-2-choose-a-model>Step 2: Choose a model</h2><p>No hypothesis class (aka model class, aka algorithm, aka estimator) is best for all data <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>. You must choose your model class based on the data. Things to consider:</p><ul><li>What&rsquo;s the dimensionality of your data?</li><li>Are your features linearly separable?</li><li>Are your features numeric or categorical?</li></ul><p>Scikit-learn calls models <em>estimators</em>.</p><p>A good first choice is a linear model class, so we&rsquo;ll use a support vector machine (SVM).</p><h2 id=step-3-set-model-parameters>Step 3: Set model parameters</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn <span style=color:#f92672>import</span> svm
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> svm<span style=color:#f92672>.</span>SVC(kernel<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;linear&#34;</span>)
</span></span></code></pre></div><p>Most parameters are optional, with reasonable default values. Beacuse we know the Iris data set is so well-suited to liner classifiers we choose a <code>linear</code> kernel (deafult is <code>rbf</code> &ndash; radial basis function)</p><h2 id=step-4-fit-model-to-data>Step 4: Fit model to data</h2><p>We want to separate our data into non-overlapping training and test subsets. Since the data in our data set are arranged in a neat order, we should randomize the samples and split in a way that represents each class equally in the training and test sets. Scikit-learn provides a library functoin to do this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span>X_iris_train, X_iris_test, y_iris_train, y_iris_test <span style=color:#f92672>=</span> \
</span></span><span style=display:flex><span>    train_test_split(X_iris,
</span></span><span style=display:flex><span>                     y_iris,
</span></span><span style=display:flex><span>                     random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p>Now we can train our classifier on the training data (fit the model to the training data).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>model<span style=color:#f92672>.</span>fit(X_iris_train, y_iris_train)
</span></span></code></pre></div><h2 id=step-5-apply-model-to-new-data>Step 5: Apply model to new data</h2><p>To apply the trained model to new (unseen) data, pass an array of instances to ~predict~:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>y_iris_model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(X_iris_test)
</span></span></code></pre></div><p>We can test the generalization error (how well the classifier performs on unseen data) using the built-in accuracy score:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> accuracy_score
</span></span><span style=display:flex><span>accuracy_score(y_iris_test, y_iris_model)
</span></span><span style=display:flex><span><span style=color:#ae81ff>1.0</span>
</span></span></code></pre></div><p>As you can see, a linear SVM classifier works perfectly on the Iris data. Try out different classifiers to see how well they perform.</p><p>A Scikit-learn estimator (model/hypothesis) is an object that has <code>fit</code> (train) and <code>predict</code> (test) methods.</p><h2 id=closing-words>Closing Words</h2><blockquote><p>&ldquo;A breakthrough in machine learning is worth 10 Microsofts.&rdquo; &ndash; Bill Gates</p></blockquote><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Slide credit: <a href=https://www.cc.gatech.edu/~bboots3/>Byron Boots</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Slide credit: Ray Mooney&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Ethem Alpaydin, Introduction to Machine Learning&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>From Andrew Ng&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Farabet et al. ICML 2012, PAMI 2013&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p><a href=https://www.cc.gatech.edu/~isbell/reading/papers/nfl-optimization.pdf>Wolpert and Macready, <em>No Free Lunch Theorems for Optimization</em></a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></main><script src=https://code.jquery.com/jquery-3.2.1.slim.min.js integrity=sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js integrity=sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q crossorigin=anonymous></script>
<script src=../js/bootstrap.min.js></script></body></html>