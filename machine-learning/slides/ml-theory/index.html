<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Dr. CS codes</title><meta name=viewport content="width=device-width"><link rel=stylesheet href=https://DrCS.codes/css/syntax.css><link rel=stylesheet href=https://DrCS.codes/css/main.css><link rel=stylesheet href=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css integrity=sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO crossorigin=anonymous><link href=https://DrCS.codes/css/navbar-top-fixed.css rel=stylesheet></head><body><nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"><a class=navbar-brand href=/machine-learning></a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbarCollapse aria-controls=navbarCollapse aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="navbar-collapse collapse" id=navbarCollapse><ul class="navbar-nav mr-auto"></ul><ul class="navbar-nav pull-right"><li class="nav-item pull-right"><a class=nav-link href=https://DrCS.codes>Dr. CS codes</a></li></ul></div></nav><main role=main class=container><p>% Computational Learning Theory</p><h2 id=decidability>Decidability</h2><ul><li><p>Computation</p><ul><li>Decidability &ndash; which problems have algorithmic solutions</li></ul></li><li><p>Machine Learning</p><ul><li>Feasibility &ndash; what assumptions must we make to trust that we can learn an unknown target function from a sample data set</li></ul></li></ul><h2 id=complexity>Complexity</h2><p>Complexity is a measure of efficiency. More efficient solutions use fewer resources.</p><ul><li><p>Computation &ndash; resources are time and space</p><ul><li>Time complexity &ndash; as a function of problem size, $n$, how many steps must an algorithm take to solve a problem</li><li>Space complexity &ndash; how much memory does an algorithm need</li></ul></li><li><p>Machine learning &ndash; resource is data</p><ul><li>Sample complexity &ndash; how many training examples, $m$, are needed so that with probability $\ge \delta$ we can learn a classifier with error rate lower than $\epsilon$</li></ul></li></ul><p>Practically speaking, computational learning theory is about how much data we need to</p><h2 id=feasibility-of-machine-learning>Feasibility of Machine Learning</h2><p>Machine learning is feasible if we adopt a probabilistic view of the problem and make two assumptions:</p><ul><li><p>Our training samples are drawn from the same (unknown) probability distribution as our test data, and</p></li><li><p>Our training samples are drawn independently (with replacement)</p></li></ul><p>These assumptions are known as the <em>i.i.d assumption</em> &ndash; data samples are independent and identically distributed (to the test data).</p><p>So in machine learning we use a data set of samples to make a statement about a population.</p><h2 id=the-hoeffding-inequality>The Hoeffding Inequality</h2><p>If we are trying to estimate some random variable $\mu$ by measuring $\nu$ in a sample set, the Hoeffding inequality bounds the difference between in-sample and out-of-sample error by</p><p>$$
\mathbb{P}[|\nu - \mu| > \epsilon] \ge 2e^{-2e^2 N}
$$</p><p>So as the number of our training samples increases, the probability decreases that our in-sample measure $\nu$ will differ from the population parameter $\mu$ it is estimating by some error tolerance $\epsilon$.</p><p>The Hoeffding inequality depends only on $N$, but this holds only for some parameter. In machine learning we are trying to estimate an entire function.</p><h2 id=the-hoeffding-inequality-in-machine-learning>The Hoeffding Inequality in Machine Learning</h2><p>In machine learning we&rsquo;re trying to learn an $h(\vec{x}) \in \mathcal{H}$ that approximates $f: \mathcal{X} \rightarrow \mathcal{Y}$.</p><ul><li>In the learning setting the measure we&rsquo;re trying to make a statement about is <em>error</em> and</li><li>we want a bound on the difference between in-sample error <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:</li></ul><p>$$
E_{in}(h) = \frac{1}{N} \sum_{n=1}^{N} \llbracket h(\vec{x}) \ne f(\vec{x}) \rrbracket
$$</p><p>and out-of-sample error:</p><p>$$
E_{out}(h) = \mathbb{P} [ h(\vec{x}) \ne f(\vec{x}) ]
$$</p><p>So the Hoeffding inequality becomes</p><p>$$
\mathbb{P} [|E_{in}(h) - E_{out}(h)| > \epsilon ] \le 2e^{-2e^2N}
$$</p><p>But this is the error for one hypothesis.</p><h2 id=error-of-a-hypothesis-class>Error of a Hypothesis Class</h2><p>We need a bound for a hypothesis class. The union bound states that if $\mathcal{B}_1, &mldr;, \mathcal{B}_M$ are any events,</p><p>$$
\mathbb{P} [ \mathcal{B}_1, or \mathcal{B}_2, or, &mldr;, or \mathcal{B}<em>M ] \le \sum</em>{m=1}^{M} \mathbb{P} [ \mathcal{B}_m ]
$$</p><p>For $\mathcal{H}$ with $M$ hypotheses ${h_1, &mldr;, h_M}$ the union bound is:</p><p>$$
\mathbb{P} [|E_{in}(g) - E_{out}(g)| > \epsilon ] \le \sum_{m=1}^{M} \mathbb{P} [|E_{in}(h(m)) - E_{out}(h(m))| > \epsilon ]
$$</p><p>If we apply the Hoeffding inequality to each of the $M$ hypotheses we get:</p><p>$$
\mathbb{P} [|E_{in}(g) - E_{out}(g)| > \epsilon ] \le 2Me^{-2 \epsilon^2 N}
$$</p><p>We&rsquo;ll return to the result later when we consider infinite hypothesis classes.</p><h2 id=epsilon-exhausted-version-spaces>$\epsilon$-Exhausted Version Spaces</h2><p>We could use the previous result to derive a formula for $N$, but there is a more convenient framework based on version spaces.</p><p>Recall that a version space is the set of all hypotheses consistent with the data.</p><ul><li>A version space is said to be $\epsilon$-exhausted with respect to the target function $f$ and the data set $\mathcal{D}$ if every hypothesis in the version space has error less than $\epsilon$ on $\mathcal{D}$.</li><li>Let $|H|$ be the size of the hypothesis space.</li><li>The probability that for a randomly chosen $\mathcal{D}$ of size $N$ the version space is not $\epsilon$-exhausted is less than</li></ul><p>$$
|H|^{-\epsilon N}
$$</p><h2 id=bounding-the-error-for-finite-mathcalh>Bounding the Error for Finite $\mathcal{H}$</h2><p>$|H|^{-\epsilon N}$ is an upper bound on the failure rate of our hypothesis class, that is, the probablility that we won&rsquo;t find hypothesis that has error less than $\epsilon$ on $\mathcal{D}$. If we want this failure rate to be no greater than some $\delta$, then</p><p>$$
|H|^{-\epsilon N} \le \delta
$$</p><p>And solving for $N$ we get</p><p>$$
N \ge \frac{1}{\epsilon} ( \ln |H| + \ln \frac{1}{\delta} )
$$</p><h2 id=pac-learning-for-finite-mathcalh>PAC Learning for Finite $\mathcal{H}$</h2><p>The PAC learning formula</p><p>$$
N \ge \frac{1}{\epsilon} ( \ln |H| + \ln \frac{1}{\delta} )
$$</p><p>means that we need at least $N$ training samples to guarantee that we will learn a hypothesis that will</p><ul><li><em>probably</em>, with probability $1 - \delta$ be</li><li><em>approximately</em>, within error $\epsilon$</li><li><em>correct</em>.</li></ul><p>Notice that $N$ grows</p><ul><li>linearly in $\frac{1}{\epsilon}$,</li><li>logarithmically in $\frac{1}{\delta}$, and</li><li>logarithmically in $|H|$.</li></ul><h2 id=pac-learning-example>PAC Learning Example</h2><p>Consider a hypothesis class of boolean literals. You have variables like <em>tall</em>, <em>glasses</em>, etc., and the hypothesis class represents whether a person will get a date. How many examples of people who did and did not get dates do you need to learn with 95% probability a hypothesis that has error no greater than .1</p><p>First, what&rsquo;s the size of the hypothesis class? For each of the variables there are three possibilities: true, false, and don&rsquo;t care. For example, one hypothesis for variables $tall, glasses, longHair$ might be:</p><p>$$
tall \land \lnot glasses \land true
$$</p><p>Meaning that you must be tall and not wear glasses to get a date but it doesn&rsquo;t matter if your hair is long.</p><h2 id=pac-learning-example-1>PAC Learning Example</h2><p>Since there are three values for each variable the size of the hypothesis class is</p><p>$$
3^d
$$</p><p>If we have 10 variables then</p><p>$$
N \ge \frac{1}{\epsilon} ( \ln |H| + \ln \frac{1}{\delta} ) = \frac{1}{.1}(\ln 3^{10} + \ln \frac{1}{.05} ) = 140
$$</p><h2 id=dichotomies>Dichotomies</h2><p>Returning to</p><p>$$
\mathbb{P} [|E_{in}(g) - E_{out}(g)| > \epsilon ] \le 2Me^{-2 \epsilon^2 N}
$$</p><p>Where $M$ is the size of the hypothesis class (also sometimes written $|H|$). For infinite hypothesis classes, this won&rsquo;t work. What we need is an <em>effective</em> number of hypotheses.</p><p>Diversity of $H$ is captured by idea of dichotomies. For a binary target function, there are many $h \in H$ that produce the same assignments of labels. We groupo these into <em>dichotomies</em>.</p><h2 id=effective-number-of-hypotheses>Effective Number of Hypotheses</h2><p><img src=magdon-effective-number-hypotheses.png alt></p><h2 id=growth-function>Growth Function</h2><p><img src=magdon-growth-function.png alt></p><h2 id=shattering>Shattering</h2><p><img src=magdon-shattering.png alt></p><h2 id=vc-dimension>VC Dimension</h2><p>The VC-dimendion $d_{VC}$ of a hypothesis set $\mathcal{H}$ is the largest $N$ for which $m_{\mathcal{H}}(N) = 2^N$.</p><p>Another way to put it: VC-dimension is the maximum number of points in a data set for which you can arrange the points in such a way that $\mathcal{H}$ shatters those points for any labellings of the points.</p><h2 id=vc-bound>VC Bound</h2><p>For a confidence $\delta > 0$, the VC generalization bound is:</p><p>$$
E_{out}(g) \le E_{in}(g) + \sqrt{ \frac{8}{N} \ln \frac{4m_\mathcal{H}(2N)}{\delta} }
$$</p><p>If we use a polynomial bound on $d_{VC}$:</p><p>$$
E_{out}(g) \le E_{in}(g) + \sqrt{ \frac{8}{N} \ln \left( \frac{4 ((2 N)^{d_{VC}} - 1}{\delta} \right) }
$$</p><h2 id=vc-bound-and-sample-complexity>VC Bound and Sample Complexity</h2><p>For an error tolerance $\epsilon > 0$ (our max acceptable difference between $E_{in}$ and $E_{out}$) and a confidence $\delta > 0$, we can compute the sample complexity of an infinite hypothesis class by:</p><p>$$
N \ge \frac{8}{\epsilon_2} \ln \left( \frac{4 ((2N)^{d_{VC}} + 1}{\delta} \right)
$$</p><p>Note that $N$ appears on both sides, so we need to solve for $N$ iteratively. See <a href=../code/colt.sc>colt.sc</a> for an example.</p><p>If we have a learning model with $d_{VC} = 3$ and want a generalization error at most $\epsilon = 0.1$ and a confidence of 90% ($\delta = 0.05$), we get $N = 29299$</p><ul><li>If we try higher values for $d_{VC}$, $N \approx 10000 d_{VC}$, which is a gross overestimate.</li><li>Rule of thumb: you need $10 d_{VC}$ training examples to get decent generalization.</li></ul><h2 id=vc-bound-as-a-penalty-for-model-complexity>VC Bound as a Penalty for Model Complexity</h2><p>You can use the VC bound to estimate the number of training samples you need, but you typically just get a data set &ndash; you&rsquo;re given an $N$.</p><ul><li>Question becomes: how well can we learn from the data given this data set?</li></ul><p>If we plug values into:</p><p>$$
E_{out}(g) \le E_{in}(g) + \sqrt{ \frac{8}{N} \ln \left( \frac{4 ((2 N)^{d_{VC}} - 1}{\delta} \right) }
$$</p><p>For $N = 1000$ and $\delta = 0.1$ we get</p><ul><li>If dvc = 1, error bound = 0.09</li><li>If dvc = 2, error bound = 0.15</li><li>If dvc = 3, error bound = 0.21</li><li>If dvc = 4, error bound = 0.27</li></ul><h2 id=appoximation-generalization-tradeoff>Appoximation-Generalization Tradeoff</h2><p>The VC bound can be seen as a penalty for model complexity. For a more complex $\mathcal{H}$ (larger $d_{VC}$), we get a larger generalization error.</p><ul><li>If $\mathcal{H}$ is too simple, it may not be able to approximate $f$.</li><li>If $\mathcal{H}$ is too complex, it may not generalize well.</li></ul><p>This tradeoff is captured in a conceptual framework called the <em>bias-variance</em> decomposition which uses squared-error to decompose the error into two terms:</p><p>$$
\mathbb{E}_{\mathcal{D}} = bias + var
$$</p><p>Which is a statement about a particular hypothesis class over all data sets, not just a particular data set.</p><h2 id=bias-variance-tradeoff>Bias-Variance Tradeoff</h2><p><img src=lfd-bias-var.png alt>{height=40%}</p><ul><li>$\mathcal{H}_1$ (on the left) are lines of the form $h(x) = b$ &ndash; high bias, low variance</li><li>$\mathcal{H}_2$ (on the right) are lines of the form $h(x) = ax + b$ &ndash; low bias, high variance</li></ul><p>Total error is a sum of errors from bias and variance, and as one goes up the other goes down. Try to find the right balance. We&rsquo;ll learn techniques for finding this balance.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>$\llbracket statement \rrbracket = 1$ when statement is true, 0 otherwise.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></main><script src=https://code.jquery.com/jquery-3.2.1.slim.min.js integrity=sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js integrity=sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q crossorigin=anonymous></script>
<script src=../js/bootstrap.min.js></script></body></html>