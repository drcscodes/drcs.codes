<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Dr. CS codes</title><meta name=viewport content="width=device-width"><link rel=stylesheet href=https://DrCS.codes/css/syntax.css><link rel=stylesheet href=https://DrCS.codes/css/main.css><link rel=stylesheet href=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css integrity=sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO crossorigin=anonymous><link href=https://DrCS.codes/css/navbar-top-fixed.css rel=stylesheet></head><body><nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"><a class=navbar-brand href=/machine-learning></a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbarCollapse aria-controls=navbarCollapse aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="navbar-collapse collapse" id=navbarCollapse><ul class="navbar-nav mr-auto"></ul><ul class="navbar-nav pull-right"><li class="nav-item pull-right"><a class=nav-link href=https://DrCS.codes>Dr. CS codes</a></li></ul></div></nav><main role=main class=container><p>% Error and Noise</p><h2 id=error-measures>Error Measures</h2><p>An error measure quantifies the performance of an $h \in \mathcal{H}$, that is, its agreement with the target function $f$.</p><p>$$
Error = E(h, f)
$$</p><p>The target function is unknown and we only have samples from it (our data set, $\mathcal{D}$), so we use a pointwise approximation. Classification error is</p><p>$$
e(h(\vec{x}), f(\vec{x})) = \llbracket h(\vec{x}) \ne f(\vec{x}) \rrbracket
$$</p><p>for some $\vec{x}$, where $\llbracket \cdot \rrbracket$ evaluates to 1 if argument is true, and to 0 if it is false.</p><h2 id=error-rate-and-accuracy-rate>Error Rate and Accuracy Rate</h2><p>Given the previous pointwise definition of error, error rate within a data set $\mathcal{D}$ can be defined as</p><p>$$
E(h) = \frac{1}{N} \sum_{n=1}^{N} \llbracket h(\vec{x}) \ne f(\vec{x}) \rrbracket
$$</p><p>In other words, it&rsquo;s the proportion of points in $\mathcal{D}$ that are misclassified by $h$. If you turn the inequality above into an equality, you have <em>accuracy</em>, that is, accuracy = $1 - E$. Some prefer to think in terms of accuracy.</p><h2 id=training-error-and-test-error>Training Error and Test Error</h2><p>The $E$ we just defined is the error of our $h$ in $\mathcal{D}$, a set of samples from $\mathcal{X}$. The book refers to this quantity as <em>in-sample</em> error, or $E_{in}$.</p><ul><li>With our data set $\mathcal{D}$ we can only deal with $E_{in}$.</li><li>What we really care about is $E_{out}$ &ndash; how will our classifier perform on any possible unseen $\vec{x}$ from $\mathcal{X}$.</li></ul><p>So in practice we separate our data set $\mathcal{D}$ into a training set and a test set.</p><ul><li>$E_{train}$ is the error rate on our training set.</li><li>$E_{test}$ is the error rate on our test set.</li></ul><p>We use $E_{test}$ as an estimate of $E_{out}$. For this estimate to be meaningful we must observe the most critical rule in practical machine learning</p><blockquote><p>You must not use any data from the test set during training.</p></blockquote><h2 id=cost>Cost</h2><p>$E$ can be thought of as the <em>cost</em> of using $h$ instead of $f$ (if you knew $f$ you&rsquo;d just use $f$). But the error measure we just defined might not be enough. Consider the case of identification by fingerprint <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:</p><p><img src=Fingerprintforcriminologystubs2.png alt>{height=5%} $\rightarrow$ $f$ $\rightarrow$ $\begin{cases}+1 \text{you}\-1 \text{not you}\end{cases}$</p><p>Is the cost of correctly identifying a person the same for all applications?</p><h2 id=kinds-of-error>Kinds of Error</h2><p>+&mdash;+&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;-+
| | | f | |
+&mdash;+&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;-+
| | | +1 | -1 |
+&mdash;+&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;-+
| h | +1 | no error | false positive |
| | -1 | false negative | no error |
+&mdash;+&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;-+&mdash;&mdash;&mdash;&mdash;&mdash;-+</p><p>Consider the following two kinds of applications:</p><ul><li>Customer identification for a supermarket discount program</li><li>Identification for authorization to enter CIA building</li></ul><p>Is the cost for each kind of error the same?</p><h2 id=cost-matrix>Cost Matrix</h2><p>We can capture the relative cost of each kind of error in a cost matrix.</p><p><img src=cost-matrix-lfd.png alt>{height=50%}</p><ul><li>Accidentally letting someone into the CIA building is 1000 times worse than accidentally rejecting someone</li><li>A learning algorithm using a cost-weighted error function will minimize the right kind of error</li></ul><h2 id=additional-error-metrics>Additional Error Metrics</h2><p>Our earlier error function didn&rsquo;t distinguish between different kinds of errors &ndash; only misclassifications.</p><p>Let</p><ul><li>$TP$ be the number of true positive predictions,</li><li>$TN$ be the number of true negative predictions,</li><li>$FP$ be the number of false positive predictions, and</li><li>$FN$ be the number of false negative predictions.</li></ul><p>Then &mldr;</p><h2 id=precision-recall-f-measure>Precision, Recall, F-measure</h2><ul><li><p>$Precision = \frac{TP}{TP + FP}$</p><ul><li>If high, a positive prediction is likely correct (good for CIA entry)</li><li>Also called &ldquo;hit rate&rdquo;</li></ul></li><li><p>$Recall = \frac{TP}{TP + FN}$</p><ul><li>If high, missed few positives but maybe had some false positives</li><li>Also called &ldquo;false alarm rate&rdquo;</li><li>Good for cancer diagnosis - better to scare someone than to miss an actual cancer</li></ul></li><li><p>$F1-score = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$</p><ul><li>If high, good precision <em>and</em> recall summarized in a single metric</li></ul></li></ul><h2 id=confusion-matrix>Confusion Matrix</h2><p>We can calculate the error metrics from a <em>confusion matrix</em>. A confusion matrix lists the counts of the different kinds of errors. We&rsquo;ve switched the positions of the true function, $f$, and our learned hypothesis, $h$, to match the output of most machine learning libraries.</p><p>Let&rsquo;s say we run a simple linear discriminant analysis on the <a href=https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29>Wisconsin Breast Cancer Diagnostic data set</a> and get the following confusoin matrix:</p><p>+&mdash;&mdash;-+&mdash;-+&mdash;&mdash;+&mdash;-+
| | | h | |
+&mdash;&mdash;-+&mdash;-+&mdash;&mdash;+&mdash;-+
| | | +1 | -1 |
+&mdash;&mdash;-+&mdash;-+&mdash;&mdash;+&mdash;-+
| truth | +1 | 48 | 7 |
+&mdash;&mdash;-+&mdash;-+&mdash;&mdash;+&mdash;-+
| | -1 | 0 | 88 |
+&mdash;&mdash;-+&mdash;-+&mdash;&mdash;+&mdash;-+</p><h2 id=evaluating-a-model-using-precision-recall-and-f-measure>Evaluating a Model using Precision, Recall, and F-measure</h2><p>+&mdash;&mdash;-+&mdash;-+&mdash;&mdash;+&mdash;-+
| | | h | |
+&mdash;&mdash;-+&mdash;-+&mdash;&mdash;+&mdash;-+
| | | +1 | -1 |
+&mdash;&mdash;-+&mdash;-+&mdash;&mdash;+&mdash;-+
| truth | +1 | 48 | 7 |
+&mdash;&mdash;-+&mdash;-+&mdash;&mdash;+&mdash;-+
| | -1 | 0 | 88 |
+&mdash;&mdash;-+&mdash;-+&mdash;&mdash;+&mdash;-+</p><p>Using these values,</p><ul><li>$Precision = \frac{TP}{TP + FP} = \frac{48}{48 + 0} = 1.0$</li><li>$Recall = \frac{TP}{TP + FN} = \frac{48}{48 + 7} = 0.87$</li><li>$F1-score = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall} = 2 \cdot \frac{1.0 \cdot 0.87}{1.0 + 0.87} = 0.93$</li></ul><p>Forget, for a moment, that this model was evaluated on a breast cancer detection data set.</p><ul><li>For what kinds of applications would this be a good result?</li><li>For what kinds of applications would this be a bad result?</li></ul><p>BTW, the simple accuracy rate for this classifier would be $\frac{48 + 88}{48 + 88 + 7} = 0.95$</p><h2 id=roc-curves>ROC Curves</h2><p>A <em>receiver operating characteristics curve</em><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, or <em>ROC</em> curve, plots the tp-rate versus the fp-rate to show the tradeoffs of a particular model on a particular data set. Here&rsquo;s a ROC curve for a linear discriminant analysis model on the breast cancer data:</p><p><img src=breast-cancer-roc.png alt>{height=50%}</p><p>How do we interpret a ROC curve? &mldr;</p><h2 id=interpreting-roc-curves>Interpreting ROC Curves</h2><p><img src=breast-cancer-roc.png alt>{height=60%}</p><ul><li>A better ROC curve will &ldquo;hug&rdquo; the upper-left corner</li><li>Area under the curve, or AUC, is a good single-number measure of overall performance.</li></ul><p>Calculating the previous metrics is straightforward, but plotting a ROC curve requires internal data used by a model. Let&rsquo;s see this in <a href=../code/breast_cancer.py>code</a> &mldr;</p><h2 id=closing-thoughts>Closing Thoughts</h2><ul><li>Error (aka cost, aka loss) is the difference between the true target function and our hypothesis.</li><li>We estimate the error with pointwise evaluations and a learning algorithm may optimize this directly.</li><li>We train a model using a training set and evaluate it (estimate true error) by calculating error on a test set.</li><li>There are two kinds of errors: false positives and false negatives.<ul><li>We can characterize the cost of the different kinds of errors for a particular application.</li></ul></li><li>Simple error or accuracy rate is a poor metric.</li><li>We can count the true positives, false positives, true negatives and false negatives in a classifiers predicitons on the test set.<ul><li>Using these counts we can calculate more fine-grained metrics for evaluating a classifier.</li><li>A ROC curve can give us a good general view of a classifier&rsquo;s general performance and tradeoffs.</li></ul></li></ul><p>and finally &mldr;</p><h2 id=the-golden-rule>The Golden Rule</h2><p>We must <em>never</em> use test data for training.</p><ul><li>Ideally we&rsquo;d have a test set unavailable to us (like in competitions).</li><li>In practice we split a data set into training and test sets during model development</li></ul><p>In case you missed it,</p><blockquote><p>WE MUST <em>NEVER</em> USE TEST DATA FOR TRAINING.</p></blockquote><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Fingerprint image by Cyrillic at the English language Wikipedia, CC BY-SA 3.0, <a href="https://commons.wikimedia.org/w/index.php?curid=3335963">https://commons.wikimedia.org/w/index.php?curid=3335963</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.646.2144">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.646.2144</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></main><script src=https://code.jquery.com/jquery-3.2.1.slim.min.js integrity=sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js integrity=sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q crossorigin=anonymous></script>
<script src=../js/bootstrap.min.js></script></body></html>