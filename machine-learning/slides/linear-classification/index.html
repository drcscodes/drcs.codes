<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Dr. CS codes</title><meta name=viewport content="width=device-width"><link rel=stylesheet href=https://DrCS.codes/css/syntax.css><link rel=stylesheet href=https://DrCS.codes/css/main.css><link rel=stylesheet href=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css integrity=sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO crossorigin=anonymous><link href=https://DrCS.codes/css/navbar-top-fixed.css rel=stylesheet></head><body><nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"><a class=navbar-brand href=/machine-learning></a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbarCollapse aria-controls=navbarCollapse aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="navbar-collapse collapse" id=navbarCollapse><ul class="navbar-nav mr-auto"></ul><ul class="navbar-nav pull-right"><li class="nav-item pull-right"><a class=nav-link href=https://DrCS.codes>Dr. CS codes</a></li></ul></div></nav><main role=main class=container><p>% Linear Classification</p><h2 id=the-linear-model>The Linear Model</h2><p>In the next few lectures we will</p><ul><li>extend the perceptron learning algorithm to handle non-linearly separable data,</li><li>explore online versis batch learning,</li><li>learn three different learning settings &ndash; classification, regression, and probability estimation</li><li>learn a fundamental concept in machine learning: gradient descent</li><li>see how the learning rate hyperparameter</li></ul><h2 id=the-linear-model-1>The Linear Model</h2><p>Recall that the linear model for binary classification is:</p><p>$$
\mathcal{H} = {h(\vec{x}) = sign(\vec{w}^T \cdot \vec{x})}
$$</p><p>where</p><p>::::{.columns valign=&ldquo;center&rdquo;}
::: {.column width=&ldquo;40%&rdquo;}</p><p>$$
\vec{w} =
\begin{bmatrix}
w_{0} \
w_{1} \
\vdots \
w_{d}
\end{bmatrix}
\in \mathbb{R}^{d + 1}
$$</p><p>:::
::: {.column width=&ldquo;40%&rdquo;}</p><p>$$
\vec{x} =
\begin{bmatrix}
1 \
x_{1} \
\vdots \
x_{d}
\end{bmatrix}
\in {1} \times \mathbb{R}^d
$$</p><p>:::
::::</p><p>Where</p><ul><li>$\vec{w} \in \mathbb{R}^{d + 1}$ where $d$ is the dimensionality of the input space and $w_0$ is a bias weight, and</li><li>$x_0 = 1$ is fixed.</li></ul><h2 id=perceptron-learning-algorithm>Perceptron Learning Algorithm</h2><p>Recall the perceptron learning algorithm, slightly reworded:</p><p>INPUT: a data set $\mathcal{D}$ with each $\vec{x_i}$ in $\mathcal{D}$ prepended with a $1$, and labels $\vec{y}$</p><ol><li><p>Initialize $\vec{w} = (w_0, w_1, &mldr;, w_d)$ with zeros or random values, $t = 1$</p></li><li><p>Receive a $\vec{x_i} \in \mathcal{D}$ for which $sign(\vec{w}^T \cdot \vec{x}) \ne y_i$</p><ul><li>Update $\vec{w}$ using the update rule:<ul><li>$\vec{w}(t + 1) \leftarrow \vec{w}(t) + y_i \vec{x_i}$</li></ul></li><li>$t \leftarrow t + 1$</li></ul></li><li><p>If there is still a $\vec{x_i} \in \mathcal{D}$ for which $sign(\vec{w}^T \cdot \vec{x}) \ne y_i$, repeat Step 2.</p></li></ol><p>TERMINATION: $\vec{w}$ is a line separating the two classes (assuming $\mathcal{D}$ is linearly separable).</p><p>Notice that the algorithm only updates the model based on a single sample. Such an algorithm is called an <em>online</em> learning algorithm.</p><p>Also remember that PLA requires that $\mathcal{D}$ be linearly separable.</p><h2 id=two-fundamental-goals-in-learning>Two Fundamental Goals In Learning</h2><p>We have two fundamental goals with our learning algorithms:</p><ul><li><p>Make $E_{out}(g)$ close to $E_{in}(g)$<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. This means that our model will generallize well. We&rsquo;ll learn how to bound the difference when we study computational learning theory.</p></li><li><p>Make $E_{in}(g)$ small. This means we have a model that fits the data well, or performs well in its prediction task.</p></li></ul><p>Let&rsquo;s now discuss how to make $E_{in}(g)$ small. We need to define <em>small</em> and we need to deal with non-separable data.</p><h2 id=non-separable-data>Non-Separable Data</h2><p>In practice perfectly linearly separable data is rare.</p><p><img src=lfd-fig3_1-linear-separability.png alt="Figure 3.1 from Learning From Data"></p><ul><li>Data set could include noise which prevents linear separablility.</li><li>Data might be fundamentally non-linearly separable.</li></ul><p>Today we&rsquo;ll learn how to deal with the first case. In a few days we&rsquo;ll learn how to deal with the second case.</p><h2 id=minimizing-the-error-rate>Minimizing the Error Rate</h2><p>Earlier in the course we said that every machine learning problem contains the following elements:</p><ul><li>An input $\vec{x}$</li><li>An unkown target function $f: \mathcal{X} \rightarrow \mathcal{Y}$</li><li>A data set $\mathcal{D}$</li><li>A learning model, which consists of<ul><li>a hypothesis class $\mathcal{H}$ from which our model comes,</li><li>a loss function that quantifies the badness of our model, and</li><li>a learning algorithm which optimizes the loss function.</li></ul></li></ul><p>Error, $E$, is another term for loss function. For the case of our simple perceptron classifer we&rsquo;re using 0-1 loss, that is, counting the errors (or proportion thereof) and our optmization procedure tries to find:</p><p>$$
\min_{\vec{w} \in \mathbb{R}^{d+1}} \frac{1}{N} \sum_{n=1}^{N} \llbracket sign(\vec{w}^T \vec{x_n}) \ne y_n \rrbracket
$$</p><p>Let&rsquo;s look at two modifications to the PLA that perform this minimization.</p><h2 id=batch-pla-alan-fern-byron-boots>Batch PLA <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></h2><p>INPUT: a data set $\mathcal{D}$ with each $\vec{x_i}$ in $\mathcal{D}$ prepended with a $1$, labels $\vec{y}$, $\epsilon$ &ndash; an error tolerance, and $\alpha$ &ndash; a learning rate</p><ol><li><p>Initialize $\vec{w} = (w_0, w_1, &mldr;, w_d)$ with zeros or random values, $t = 1$, $\Delta = (0, ..0)$</p></li><li><p>do</p><ul><li><p>For $i = 1, 2, &mldr;, N$</p><ul><li>if $sign(\vec{w}^T \cdot \vec{x_i}) \ne y_i$, then $\Delta \leftarrow \Delta + y_i \vec{x_i}$</li><li>$\Delta \leftarrow \frac{\Delta}{N}$</li></ul></li><li><p>$\vec{w} \leftarrow \vec{w} + \alpha \Delta$</p></li></ul><p>while $||\Delta||_{2} > \epsilon$</p></li></ol><p>TERMINATION: $\vec{w}$ is &ldquo;close enough&rdquo; a line separating the two classes.</p><h2 id=new-concepts-in-the-batch-pla-algorithm>New Concepts in the Batch PLA Algorithm</h2><ol><li><p>Initialize $\vec{w} = (w_0, w_1, &mldr;, w_d)$ with zeros or random values, $t = 1$, $\Delta = (0, ..0)$</p></li><li><p>do</p><ul><li><p>For $i = 1, 2, &mldr;, N$</p><ul><li>if $sign(\vec{w}^T \cdot \vec{x_i}) \ne y_i$, then $\Delta \leftarrow \Delta + y_i \vec{x_i}$</li><li>$\Delta \leftarrow \frac{\Delta}{N}$</li></ul></li><li><p>$\vec{w} \leftarrow \vec{w} + \alpha \Delta$</p></li></ul><p>while $||\Delta||_{2} > \epsilon$</p></li></ol><p>Notice a few new concepts in the batch PLA algorithm:</p><ul><li>the inner loop. This is a <em>batch algorithm</em> &ndash; it uses every sample in the data set to update the model.</li><li>the $\epsilon$ hyperparameter &ndash; our stopping condition is &ldquo;good enough&rdquo;, i.e., within an error tolerance</li><li>the $\alpha$ (also sometimes $\eta$) hyperparameter &ndash; the learning rate, i.e., how much do we update the model in a given step.</li></ul><h2 id=pocket-algorithm>Pocket Algorithm</h2><p>Input: a data set $\mathcal{D}$ with each $\vec{x_i}$ in $\mathcal{D}$ prepended with a $1$, labels $\vec{y}$, and $T$ steps</p><ol><li><p>Initialize $\vec{w} = (w_0, w_1, &mldr;, w_d)$ with zeros or random values, $t = 1$, $\Delta = (0, ..0)$</p></li><li><p>for $t = 1, 2, &mldr;, T$</p><ul><li><p>Run PLA for one update to obtain $\vec{w}(t + 1)$</p></li><li><p>Evaluate $E_{in}(\vec{w}(t + 1))$</p><ul><li>if $E_{in}(\vec{w}(t + 1)) &lt; E_{in}(\vec{w}(t))$, then $\vec{w} \leftarrow \vec{w}(t + 1)$</li></ul></li></ul></li><li><p>On termination, $\vec{w}$ is the best line found in T steps.</p></li></ol><p>Notice</p><ul><li>there is an inner loop under Step 2 to evaluate $E_{in}$. This is also a <em>batch algorithm</em> &ndash; it uses every sample in the data set to update the model.</li><li>the $T$ hyperparameter simply sets a hard limit on the number of learning iterations we perform</li></ul><h2 id=features>Features</h2><p>Remember that the target function we&rsquo;re trying to learn has the form $f: \mathcal{X} \rightarrow \mathcal{Y}$, where $\mathcal{X}$ is typically a matrix of feature vectors and $\mathcal{Y}$ is a vector of corresponding labels (classes). Consider the problem of classifying images of hand-written digits:</p><p><img src=digit-images.png alt>{height=50%}</p><p>What should the feature vector be?</p><h2 id=feature-engineering>Feature Engineering</h2><p>Sometimes deriving descriptive features from raw data can improve the performance of machine learning algorithms.</p><p><img src=intesity-symmetry-5-1.png alt>{height=50%}<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><p>Here we project the 256 features of the digit images (more if you consider pixel intensity) into a 2-dimensional space: average intensity and symmetry.</p><h2 id=multiclass-classification>Multiclass Classification</h2><p>We&rsquo;ve only discussed binary classifers so far. How can we deal with a multiclass problem, e.g., 10 digits?</p><ul><li>Some classifiers can do multi-class classification (e.g., multinomial logistic regression).</li><li>Binary classifiers can be combined in a chain to handle multiclass problems</li></ul><pre tabindex=0><code>digraph foo {
  rankdir=LR;
  b1 [shape=box, label=&#34;Binary Classifier 1&#34;];
  b2 [shape=box, label=&#34;Binary Classifier 2&#34;];
  x [shape=none];
  Class1 [shape=none];
  NotClass1 [shape=none];
  Class2 [shape=none];
  NotClass2 [shape=none];
  
  x -&gt; b1 [arrowhead=vee, arrowtail=none, dir=both, tailclip=false];
  b1 -&gt; Class1 [arrowhead=vee, arrowtail=none, dir=both, tailclip=true];
  b1 -&gt; NotClass1  [arrowhead=vee, arrowtail=none, dir=both, tailclip=true];
  x -&gt; b2 [arrowhead=vee, arrowtail=none, dir=both, tailclip=false];
  b2 -&gt; Class2 [arrowhead=vee, arrowtail=none, dir=both, tailclip=true];
  b2 -&gt; NotClass2 [arrowhead=vee, arrowtail=none, dir=both, tailclip=true];
}
</code></pre><p>This is a simple example of an ensemble, which we&rsquo;ll discuss in greater detail in the second half of the course.</p><h2 id=closing-thoughts>Closing Thoughts</h2><ul><li><p>Most data sets are not linearly separable</p><ul><li>We minimize some error, or loss function</li></ul></li><li><p>Learning algorithms learn in one of two modes:</p><ul><li>Online learning algorithm &ndash; model is updated after seeing one training samples</li><li>Batch learning algorithm &ndash; model is updated after seeing all training samples</li></ul></li><li><p>We&rsquo;ve now seen hyperparamters to tune the operation of learning algorithms</p><ul><li>$T$ or $\epsilon$ to bound the number of learning iterations</li><li>A learning rate, $\alpha$ or $\eta$, to modulate the step size of the model update performed in each iteration</li></ul></li><li><p>A multiclass classification problem can be solved by a chain of binary classifiers</p></li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Remember that a <em>version space</em> is the set of all $h$ in $\mathcal{H}$ consistent with our training data. $g$ is the particular $h$ chosen by the algorithm.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Based on Alan Fern via Byron Boots&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=http://www.cs.rpi.edu/~magdon/courses/learn/slides.html>http://www.cs.rpi.edu/~magdon/courses/learn/slides.html</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></main><script src=https://code.jquery.com/jquery-3.2.1.slim.min.js integrity=sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js integrity=sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q crossorigin=anonymous></script>
<script src=../js/bootstrap.min.js></script></body></html>