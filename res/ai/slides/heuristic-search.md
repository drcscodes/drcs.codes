---
title: Artificial Intelligence
subtitle: Heuristic Search
author: Christopher Simpkins
aspectratio: 1610
fontsize: 10pt
colorlinks: yes
urlcolor: blue
header-includes:
- |
    ```{=latex}
    \input{beamer-common}
    ```
---

## Informed (Heuristic) Search Strategies

- Use domain-specific hints about "distance" from goals
- Hints encapsulated in a **heuristic function**, $h(node)$:

    - $h(node)$ = estimated cost of cheapest path from $node$ to a goal state
    - $h$ is really a function of $state$, not $node$.  We use $h(node)$ to be consistent with $f(node)$ in best-first search, and path cost, $g(node)$.
    - Book uses $f(n)$, $g(n)$ and $h(n)$.  I use $node$ instead of $n$ to clearly distinguish from $n$ as an index in problem size, $N$.

Example Heuristic for Romania, $h_{SLD}$:

```{=latex}
\begin{center}
```
![](aima-fig-03_16-straight-line-distances.pdf)
```{=latex}
\end{center}
```

Straight line distances to Bucharest from each of the cities in Romania.

## Greedy Best-First Search

- Recall that best-first search uses a priority queue for its frontier, ordered by $f(node)$
- Greedy best-first search uses $f(node) = h(node)$
- Greediness: get as close to the goal as possible in each step

```{=latex}
\begin{center}
```
![](aima-fig-03_17-greedy-best-first-progress.pdf){height="70%"}
```{=latex}
\end{center}
```

## Optimality of Greedy Best-First Search

```{=latex}
\begin{center}
```
![](aima-fig-03_10-sibiu-bucharest.pdf){height="50%"}
```{=latex}
\end{center}
```

- Greedy best-first search returns the path via Sibiu and Fagaras to Bucharest.
- The path through Rimnicu Vilcea and Pitesti is 32 miles shorter.

## $A^*$ Search

$$
f(node) = g(node) + h(node)
$$

- Complete
- Optimal with an admissible heuristic
- Relatively efficient, but can generate exponential number of nodes for some problems.

Heavily dependent on quality of heuristic function.

## $A^*$ Progress Part 1

```{=latex}
\begin{center}
```
![](aima-fig-03_18-astar-progress-1.pdf){height="90%"}
```{=latex}
\end{center}
```

## $A^*$ Progress Part 2

```{=latex}
\begin{center}
```
![](aima-fig-03_18-astar-progress-2.pdf)
```{=latex}
\end{center}
```

## Admissibility and Consistency

- An **admissible** heuristic never overestimates the cost to reach a goal.
- A **consistent** heuristic is a kind of local admissibility: for every node $node$ and successor $node'$ generated by action $a$: $h(node) \le c(node, a, node') + h(n')$.  This is a form of **triangle inequality**.

```{=latex}
\begin{center}
```
![](aima-fig-03_19-consistency-triangle-inequality.pdf)
```{=latex}
\end{center}
```

- Admissibility is required to guarantee cost-optimality in $A^*$.
- Consistency improves performance by guaranteeing that the first time we reach a node, it is on the optimal path -- so we don't re-evaluate multiple paths to the same node.

## Search Contours

- In a topographical map, countours indicate a constant elevation
- In a search contour of a state space, a contour indicates an upper bound on path cost in a region

    - In the 400 countour, each node has $f(node) = g(node) + h(node) \le 400$.

```{=latex}
\begin{center}
```
![](aima-fig-03_20-astar-contours.pdf)
```{=latex}
\end{center}
```

- Countours represent monotonically increasing path costs.

## Satisficing Search: $A^*$ vs Weighted $A^*$

- Detour index: multiplier applied to straight-line distance to account for curvature of roads.  E.g., detour index of 1.3 means a road connecting locations 10 miles apart would be estimated as 13 miles long.
- Weighted $A^*$ search: apply a wieght, like detour index, to $h(node)$

    - $f(n) = g(n) + w \cdot h(n)$, for some $w > 1$

- Results in inadmissible heuristic (overestimates), but can improve search speed.


```{=latex}
\begin{center}
```
![](aima-fig-03_21-astar-vs-weighted-astar.png){height="30%"}
```{=latex}
\end{center}
```

(a) an $A^*$ search and (b) a weighted $A^*$ search with weight $w = 2$.

- The gray bars are obstacles, the purple line is the path from the green start to red goal, and the small dots are states that were reached by each search.
- On this particular problem, weighted $A^*$ explores 7 times fewer states and finds a path that is 5% more costly.

## Memory-Bounded Search

$A^*$ is not memory-efficient.  Some approaches to imporoving memory efficiency:

- **Beam search**  keeps only the $k$ nodes with lowest $f$ values.

    - Forms a narrow "beam" through the search space.
    - Not complete or optimal, but good enough with sufficiently large $k$
    - Alternative: keep nodes within $\sigma$ of best $f$ score, so only narrow beam when there are clearly better nodes.

- **Iterative-deepening $A^*$** uses $f = g + h$ as the cut-off for the frontier instead of depth.

    - Iteratively expands contours of search space.

- **Recursive best-first search** resembles depth-first search.

    - Instead of continuing down a path indefinitely, keeps track of path with second best $f$ value of ancestor.  If that $f$ value is exceeded, discards path and backs up to the alternative path.
    - $f$ value of discarded path is kept in case alternative doesn't work out.

- **Simplified memory-bounded $A^*$ ($SMA^*$)** is similar to RBFS, but expands best leaf node until memory is full.  Then it discards the worst leaf and continues.

## Heuristic Functions

```{=latex}
\begin{center}
```
![](aima-fig-03_03-eight-puzzle.pdf)
```{=latex}
\end{center}
```

- Misplaced tiles, $h_1 = 8$.
- Manhattan distance, $h_2 = 3 + 1 + 2 + 2 + 2 + 3 + 3 + 2= 18$

True solution cost is 26, so neither heristic overestimates.

## Heuristic Accuracy and Performance

- Effective branching factor, $b^*$: for $N$ nodes, branching factor of uniform tree of depth $d$ that would contain $N+1$ nodes.  Want close to 1.

    - $N + 1 = 1 + b^* + (b^*)^2 + \cdots + (b^*)^d$

```{=latex}
\begin{center}
```
![](aima-fig-03_26-eight-puzzle-heuristics-comparisons.pdf)
```{=latex}
\end{center}
```


- $h_2$ dominates $h_1$ because for any $node$, $h_2(node) \ge h_1(node)$
- We want a heuristic that underestimates, but by as little as possible.

## Designing Heuristic Functions

- Relaxing the problem definition
- Storing precomputed solution costs for subproblems in a pattern database
- Defining landmarks
- Learning from experience

Designing heuristic functions requires domain knowledge.  But there is an automated approach based on relaxed problem definitions: [`Absolver`](https://www.ijcai.org/Proceedings/91-2/Papers/017.pdf)
