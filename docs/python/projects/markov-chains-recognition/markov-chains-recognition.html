<!DOCTYPE html>
<html lang="en">
<head>
    <title>Markov Chains for Recognition</title>
    <link rel="stylesheet" href="../../../assets/css/main.css">
    <link rel="stylesheet" href="../../../assets/css/codehilite.css">
</head>
<body>
    <a class="button" onclick="history.back()"><< Back to course</a>
    <div class="main">
    <h1>Markov Chains for Recognition</h1>
<h2>Introduction</h2>
<p>In this assignment you'll practice</p>
<ul>
<li>writing classes and modules,</li>
<li>using multi-dimensional arrays,</li>
<li>simple text processing, and</li>
<li>basic numerical computing issues in Python.</li>
</ul>
<h2>Problem Description</h2>
<p>You're interested in natural language processing, and in the problem of identifying the source of a text.</p>
<h2>Solution Description</h2>
<p>Write a module named <code>source.py</code> with a class named <code>SourceModel</code> whose constructor takes a name for a source, and a corpus object of type <code>TextIOWrapper</code> (such as a file object -- see <a href="https://docs.python.org/3/library/io.html">io module</a>) and builds a first-order Markov model of the transitions between letters in the source. Only alphabetic characters in the source corpus should be considered, and they should be normalized to upper or lower case. For simplicity (see background) only consider the 26 letters of the English alphabet (for languages whose alphabets have diacritics, simply drop them, e.g., use e for é, u for ü, etc.).</p>
<p>Here are some example corpus files and test files:</p>
<ul>
<li>English: <a href="english.corpus">english.corpus</a>, <a href="english.test">english.test</a></li>
<li>French: <a href="french.corpus">french.corpus</a>, <a href="french.test">french.test</a></li>
<li>Spanish: <a href="spanish.corpus">spanish.corpus</a>, <a href="spanish.test">spanish.test</a></li>
<li>HipHop: <a href="hiphop.corpus">hiphop.corpus</a>, <a href="hiphop.test">hiphop.test</a></li>
<li>Lisp: <a href="lisp.corpus">lisp.corpus</a>, <a href="lisp.test">lisp.test</a></li>
</ul>
<p>You can assume corpus files are of the form <code>&lt;source-name&gt;.corpus</code>.</p>
<h3>Background</h3>
<p>In machine learning we train a model on some data and then use that model to make predictions about unseen instance of the same kind of data. For example, we can train a machine learning model on a data set consisting of several labeled images, some of which depict a dog and some of which don't. We can then use the trained model to predict whether some unseen image (an image not in the training set) has a dog. The better the model, the better the accuracy (percentage of correct predictions) on unseen data.</p>
<p>We can create a model of language and use that model to predict the likelihood that some unseen text was generated by that model, in other words, how likely the unseen text is an example of the language modeled by the model. The model could be of a particular author, or a language such as French or English. One simple kind of model is well-suited to this problem: Markov models.</p>
<p>Markov models are useful for time-series or other sequential data. A Markov model is a finite-state model in which the current state is dependent only on a bounded history of previous states.  In a first-order Markov model the current state is dependent on only one previous state.</p>
<p>One can construct a simple first-order Markov model of a language as the transition probabilities between letters in the language's alphabet. For example, given this training corpus of a language:</p>
<div class="codehilite"><pre><span></span><code>BIG A, little a, what begins with A?
Aunt Annie’s alligator.
A...a...A

BIG B, little b, what begins with B?
Barber, baby, bubbles and a bumblebee.
</code></pre></div>
<p>We would have the model:</p>
<p><img alt="Dr Seuss Language Model" src="drseuss-model.png" /></p>
<p>We've only shown the letter-to-letter transitions that occur in the training corpus. Notice:</p>
<ul>
<li>we've normalized to lowercase,</li>
<li>we only consider letters -- whitespace and punctuation are ignored</li>
<li>the transition probabilities from one letter to all other letters sum to 1.0 (approximately, due to rounding),</li>
<li>in a complete model, there would be arrows leading from all letters to all other letters, for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mn>26</mn><mn>2</mn></msup><mn>6</mn></mrow></math> edges in a graph of 26 nodes, one for each letter, with unseen transitions labeled with 0.</li>
</ul>
<p>This model indicates that whenever the letter a occurs in our training corpus, the next letter is a, b, e, g, h, i, l, n, r, s, t, u or w. The arrow from a to b is labeled .19 because b appears after a 3 out of 16 times, or approximately 19 percent of the time. A first-order Markov chain is a kind of bigram model. Here are all the bigrams in the training text that begin with a, that is, all the state transitions from a:</p>
<div class="codehilite"><pre><span></span><code>(a, l), (a, w), (a, t), (a, a), (a, u), (a, n), (a, l), (a, t),
(a, a), (a, a), (a, b), (a, t), (a, r), (a, b), (a, n), (a, b)
</code></pre></div>
<p>A Markov chain represents all the bigrams and their probabilities of occurrence in the training corpus.</p>
<h4>Representation as a matrix.</h4>
<p>A Markov chain can be represented as a transition matrix in which the probability of state j after state i is found in element (i, j) of the matrix. In the example below we have labeled the rows and columns with letters for readability. The probability of seeing the letter n after the letter a in the training corpus is found by entering row a and scanning accross to column n, where we find the probability .12.</p>
<table>
<thead>
<tr>
<th></th>
<th>a</th>
<th>b</th>
<th>c</th>
<th>d</th>
<th>e</th>
<th>f</th>
<th>g</th>
<th>h</th>
<th>i</th>
<th>j</th>
<th>k</th>
<th>l</th>
<th>m</th>
<th>n</th>
<th>o</th>
<th>p</th>
<th>q</th>
<th>r</th>
<th>s</th>
<th>t</th>
<th>u</th>
<th>v</th>
<th>w</th>
<th>x</th>
<th>y</th>
<th>z</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>a</strong></td>
<td>0.19</td>
<td>0.19</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.12</td>
<td>0.01</td>
<td>0.12</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.06</td>
<td>0.01</td>
<td>0.19</td>
<td>0.06</td>
<td>0.01</td>
<td>0.06</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>b</strong></td>
<td>0.12</td>
<td>0.12</td>
<td>0.01</td>
<td>0.01</td>
<td>0.24</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.12</td>
<td>0.01</td>
<td>0.01</td>
<td>0.18</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.12</td>
<td>0.01</td>
<td>0.06</td>
<td>0.01</td>
<td>0.06</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>c</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>d</strong></td>
<td>1.00</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>e</strong></td>
<td>0.11</td>
<td>0.22</td>
<td>0.01</td>
<td>0.01</td>
<td>0.11</td>
<td>0.01</td>
<td>0.22</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.11</td>
<td>0.22</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>f</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>g</strong></td>
<td>0.40</td>
<td>0.20</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.40</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>h</strong></td>
<td>0.75</td>
<td>0.25</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>i</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.10</td>
<td>0.01</td>
<td>0.30</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.20</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.40</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>j</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>k</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>l</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.50</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.38</td>
<td>0.01</td>
<td>0.01</td>
<td>0.12</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>m</strong></td>
<td>0.01</td>
<td>1.00</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>n</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.17</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.17</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.17</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.33</td>
<td>0.17</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>o</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>1.00</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>p</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>q</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>r</strong></td>
<td>0.33</td>
<td>0.67</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>s</strong></td>
<td>0.50</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.50</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>t</strong></td>
<td>0.10</td>
<td>0.20</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.20</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.20</td>
<td>0.01</td>
<td>0.01</td>
<td>0.10</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.20</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>u</strong></td>
<td>0.01</td>
<td>0.33</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.33</td>
<td>0.33</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>v</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>w</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.50</td>
<td>0.50</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>x</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>y</strong></td>
<td>0.01</td>
<td>1.00</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>z</strong></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
</tbody>
</table>
<h4>Prediction using a Markov Chain Model.</h4>
<p>Given a Markov chain model of a source, we can compute the probability that the model would produce a given string of letters by applying the chain rule. Simply stated, we walk the transitions in the Markov chain and multiply the transition probabilities. For example, to compute the probability that "Big C, Little C" would be produced by our model, we would get the following probabilities from the transition matrix and compute their product:</p>
<div class="codehilite"><pre><span></span><code>p(b, i) = .12
p(i, g) = .30
p(g, c) = .01
p(c, l) = .01
p(l, i) = .38
p(i, t) = .40
p(t, t) = .20
p(t, l) = .20
p(l, e) = .50
p(e, c) = .01
</code></pre></div>
<p>Multiplying them gives us 1.0588235294117648e-10. Notice that, in order to avoid getting zero-probability predictions using our simplified technique, we store .01 in our transition matrix for any bigram we don't see in the training corpus.</p>
<blockquote>
<p>Note: for longer texts that would require long chains of multiplication it is possible to get floating-point underflow since each probability is &lt; 1.  There are ways to avoid this, but numerical computation is beyond the scope of this course.  For this exercise, stick to shorter query sentences.</p>
</blockquote>
<h4>Additional Information</h4>
<p>We've greatly simplified the presentation here to focus on the programming. For more information consult the following references.</p>
<ul>
<li>Natural language processing: https://web.stanford.edu/~jurafsky/slp3/</li>
<li>Markov chains: Chapter 11 of http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book.html, direct link: https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf</li>
</ul>
<h3>Requirements</h3>
<p>Here's a skeleton <code>source.py</code> to get you started:</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">SourceModel</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">text_stream</span><span class="p">):</span>
        <span class="c1"># Recommended algorithm for building your transition matrix:</span>
        <span class="c1"># Initialize a 26x26 matrix (e.g., 26-element list of 26-element lists)</span>
        <span class="c1"># Print &quot;Training {lang} model ... &quot;</span>
        <span class="c1"># Read the text_stream one character at a time.</span>
        <span class="c1"># For each character, increment the corresponding (row, col) in your</span>
        <span class="c1"># matrix. The row is the for the previous character, the col is for</span>
        <span class="c1"># the current character. (You could also think of this in terms of bigrams.)</span>
        <span class="c1"># After you read the entire text_stream, you&#39;ll have a matrix of counts.</span>
        <span class="c1"># From the matrix of counts, create a matrix of probabilities --</span>
        <span class="c1"># each row of the transition matrix is a probability distribution.</span>
        <span class="c1"># Print &quot;done.&quot;</span>
    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">probability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_stream</span><span class="p">):</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
   <span class="c1"># The first n-1 arguments to the script are corpus files to train models.</span>
   <span class="c1"># Corupus files are of the form &lt;source-name&gt;.corpus</span>

   <span class="c1"># The last argument to the script is either a file to analyze or a sentence.</span>
   <span class="c1"># Sentences should be in quotes. (Same with file names that have special characters</span>
   <span class="c1"># or spaces, but decent people don&#39;t put special characters or spaces in file names.)</span>

   <span class="c1"># Create a SourceModel object for each corpus</span>

   <span class="c1"># Use the models to compute the probability that the test text was produced by the model</span>

   <span class="c1"># Probabilities will be very small. Normalize the probabilities of all the model</span>
   <span class="c1"># predictions to a probability distribution (so they sum to 1)</span>
   <span class="c1"># (closed-world assumption -- we only state probabilities relative to models we have).</span>

   <span class="c1"># Print results of analysis, sorted in order from most likely to least likely</span>
</code></pre></div>
<h2>Running Your Script</h2>
<p>Here's an analysis of a line from a famous rap song:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>python<span class="w"> </span>source.py<span class="w"> </span>*.corpus<span class="w"> </span><span class="s2">&quot;If you got a gun up in your waist please don&#39;t shoot up the place (why?)&quot;</span>
Training<span class="w"> </span>english<span class="w"> </span>model<span class="w"> </span>...<span class="w"> </span><span class="k">done</span>.
Training<span class="w"> </span>french<span class="w"> </span>model<span class="w"> </span>...<span class="w"> </span><span class="k">done</span>.
Training<span class="w"> </span>hiphop<span class="w"> </span>model<span class="w"> </span>...<span class="w"> </span><span class="k">done</span>.
Training<span class="w"> </span>lisp<span class="w"> </span>model<span class="w"> </span>...<span class="w"> </span><span class="k">done</span>.
Training<span class="w"> </span>spanish<span class="w"> </span>model<span class="w"> </span>...<span class="w"> </span><span class="k">done</span>.
Analyzing<span class="w"> </span>string<span class="w"> </span>If<span class="w"> </span>you<span class="w"> </span>got<span class="w"> </span>a<span class="w"> </span>gun<span class="w"> </span>up<span class="w"> </span><span class="k">in</span><span class="w"> </span>your<span class="w"> </span>waist<span class="w"> </span>please<span class="w"> </span>don<span class="err">&#39;</span>t<span class="w"> </span>shoot<span class="w"> </span>up<span class="w"> </span>the<span class="w"> </span>place<span class="w"> </span><span class="o">(</span>why?<span class="o">)</span>.
Relative<span class="w"> </span>probability<span class="w"> </span>that<span class="w"> </span><span class="nb">test</span><span class="w"> </span>string<span class="w"> </span>is<span class="w"> </span>hiphop<span class="w">  </span>:<span class="w"> </span><span class="m">0</span>.998103661.
Relative<span class="w"> </span>probability<span class="w"> </span>that<span class="w"> </span><span class="nb">test</span><span class="w"> </span>string<span class="w"> </span>is<span class="w"> </span>english<span class="w"> </span>:<span class="w"> </span><span class="m">0</span>.001896339.
Relative<span class="w"> </span>probability<span class="w"> </span>that<span class="w"> </span><span class="nb">test</span><span class="w"> </span>string<span class="w"> </span>is<span class="w"> </span>french<span class="w">  </span>:<span class="w"> </span><span class="m">0</span>.000000000.
Relative<span class="w"> </span>probability<span class="w"> </span>that<span class="w"> </span><span class="nb">test</span><span class="w"> </span>string<span class="w"> </span>is<span class="w"> </span>spanish<span class="w"> </span>:<span class="w"> </span><span class="m">0</span>.000000000.
Relative<span class="w"> </span>probability<span class="w"> </span>that<span class="w"> </span><span class="nb">test</span><span class="w"> </span>string<span class="w"> </span>is<span class="w"> </span>lisp<span class="w">    </span>:<span class="w"> </span><span class="m">0</span>.000000000.
</code></pre></div>
<p>And here's a line from a popular French pop song:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>python<span class="w"> </span>source.py<span class="w"> </span>*.corpus<span class="w"> </span><span class="s2">&quot;Ou va le monde&quot;</span>
Training<span class="w"> </span>english<span class="w"> </span>model<span class="w"> </span>...<span class="w"> </span><span class="k">done</span>.
Training<span class="w"> </span>french<span class="w"> </span>model<span class="w"> </span>...<span class="w"> </span><span class="k">done</span>.
Training<span class="w"> </span>hiphop<span class="w"> </span>model<span class="w"> </span>...<span class="w"> </span><span class="k">done</span>.
Training<span class="w"> </span>lisp<span class="w"> </span>model<span class="w"> </span>...<span class="w"> </span><span class="k">done</span>.
Training<span class="w"> </span>spanish<span class="w"> </span>model<span class="w"> </span>...<span class="w"> </span><span class="k">done</span>.
Analyzing<span class="w"> </span>string<span class="w"> </span>Ou<span class="w"> </span>va<span class="w"> </span>le<span class="w"> </span>monde.
Relative<span class="w"> </span>probability<span class="w"> </span>that<span class="w"> </span><span class="nb">test</span><span class="w"> </span>string<span class="w"> </span>is<span class="w"> </span>french<span class="w">  </span>:<span class="w"> </span><span class="m">0</span>.904369735.
Relative<span class="w"> </span>probability<span class="w"> </span>that<span class="w"> </span><span class="nb">test</span><span class="w"> </span>string<span class="w"> </span>is<span class="w"> </span>lisp<span class="w">    </span>:<span class="w"> </span><span class="m">0</span>.064067191.
Relative<span class="w"> </span>probability<span class="w"> </span>that<span class="w"> </span><span class="nb">test</span><span class="w"> </span>string<span class="w"> </span>is<span class="w"> </span>english<span class="w"> </span>:<span class="w"> </span><span class="m">0</span>.014523700.
Relative<span class="w"> </span>probability<span class="w"> </span>that<span class="w"> </span><span class="nb">test</span><span class="w"> </span>string<span class="w"> </span>is<span class="w"> </span>hiphop<span class="w">  </span>:<span class="w"> </span><span class="m">0</span>.009526881.
Relative<span class="w"> </span>probability<span class="w"> </span>that<span class="w"> </span><span class="nb">test</span><span class="w"> </span>string<span class="w"> </span>is<span class="w"> </span>spanish<span class="w"> </span>:<span class="w"> </span><span class="m">0</span>.007512493.
</code></pre></div>

    </div>
</body>
</html>