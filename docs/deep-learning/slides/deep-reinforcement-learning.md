---
title: Deep Reinforcement Learning
author: CS 4277 Deep Learning
institute: Kennesaw State University
aspectratio: 1610
fontsize: 10pt
colorlinks: yes
urlcolor: blue
header-includes:
- |
  ```{=latex}
  \input{beamer-common}
  %\titlegraphic{\includegraphics[width = 1in]{chris-simpkins-headshot-320px}}
  \usepackage{framed}
  \usepackage{xcolor}
  \usepackage{tikz,pgfplots}
  \let\oldquote=\quote
  \let\endoldquote=\endquote
  \colorlet{shadecolor}{cyan!15}
  \renewenvironment{quote}{\begin{shaded*}\begin{oldquote}}{\end{oldquote}\end{shaded*}}
  ```
---

## Reinforcement Learning

```{=latex}
\begin{center}
```
![](./ReinforceMDPLoop.pdf)
```{=latex}
\end{center}
```


## Markov Process

```{=latex}
\begin{center}
```
![](./ReinforceMDP.pdf)
```{=latex}
\end{center}
```

## Markov Reward Process

```{=latex}
\begin{center}
```
![](./ReinforceMDP2.pdf)
```{=latex}
\end{center}
```

## Markov Decision Process

```{=latex}
\begin{center}
```
![](./ReinforceMDP3.pdf)
```{=latex}
\end{center}
```

## Parially-Observable MDP (POMDP)

```{=latex}
\begin{center}
```
![](./ReinforcePOMDP.pdf)
```{=latex}
\end{center}
```

## Policy

```{=latex}
\[
\pi(a|s)
\]
```

## State and Action Values

```{=latex}
\begin{center}
```
![](./ReinforceValueOfAction.pdf)
```{=latex}
\end{center}
```

## Optimal Policy

```{=latex}
\[
\pi(a|s)
\]
```


## Bellman Equations

```{=latex}
\begin{center}
```
![](./ReinforceBellman3.pdf)
```{=latex}
\end{center}
```

## Bellman Equations 2

```{=latex}
\begin{center}
```
![](./ReinforceBellman2.pdf)
```{=latex}
\end{center}
```

## Tabular RL

```{=latex}
\begin{center}
```
![]()
```{=latex}
\end{center}
```

## Dynamic Programming

```{=latex}
\begin{center}
```
![](./ReinforceDP.pdf)
```{=latex}
\end{center}
```

## Policy Iteration

```{=latex}
\[
\pi(a|s)
\]
```

## Value Iteration

```{=latex}
\[
\pi(a|s)
\]
```

## Monte Carlo Methods

```{=latex}
\begin{center}
```
![](./ReinforceMC.pdf)
```{=latex}
\end{center}
```

## Temporal Difference Methods

```{=latex}
\begin{center}
```
![](./ReinforceTD.pdf)
```{=latex}
\end{center}
```

## Fitted Q-Learning

```{=latex}
\[
\pi(a|s)
\]
```


## Deep Q-Networks for Playing Atari Games

```{=latex}
\begin{center}
```
![](./ReinforceDQL.pdf)
```{=latex}
\end{center}
```

## DQN Architecture

```{=latex}
\begin{center}
```
![](./ReinforceDQL2.pdf)
```{=latex}
\end{center}
```

## Double Q-Learning and Double Deep Q-Networks


```{=latex}
\[
\pi(a|s)
\]
```

## Policy Gradient Methods

```{=latex}
\[
\pi(a|s)
\]
```

```{=latex}
\begin{center}
```
![](./ReinforcePolicyGrad.pdf)
```{=latex}
\end{center}
```

## REINFORCE Algorithm

```{=latex}
\[
\pi(a|s)
\]
```

## Baselines

```{=latex}
\begin{center}
```
![](./ReinforcementVariance.pdf)
```{=latex}
\end{center}
```

## Actor-Critic Methods

```{=latex}
\begin{center}
```
![]()
```{=latex}
\end{center}
```

## Offline Reinforcement  Learning

```{=latex}
\begin{center}
```
![](./ReinforceDecisionTransformer.pdf)
```{=latex}
\end{center}
```


## Closing Thoughts

Boom!
